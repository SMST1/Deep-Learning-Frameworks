{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is TensorFlow 2.0, and how is it different from TensorFlow 1.x?\n",
        "\n",
        "TensorFlow 2.0 is an open-source machine learning framework developed by Google, designed for:\n",
        "\n",
        "Building deep learning models\n",
        "\n",
        "Running computations on CPUs, GPUs, and TPUs\n",
        "\n",
        "Supporting production, research, and prototyping workflows\n",
        "\n",
        "TensorFlow 2.0 focuses on simplicity, ease of use, and performance, with better compatibility across platforms (desktop, mobile, web, cloud)\n",
        "\n",
        "Key Differences Between TensorFlow 2.0 and 1.x:\n",
        "\n",
        "| Feature                    | TensorFlow 1.x                                  | TensorFlow 2.0                       |\n",
        "| -------------------------- | ----------------------------------------------- | ------------------------------------ |\n",
        "| **Execution Mode**         | Graph mode (static computation)                 | Eager execution (dynamic by default) |\n",
        "| **Ease of Use**            | Complex APIs, steep learning curve              | Cleaner, more Pythonic API           |\n",
        "| **Keras Integration**      | Optional (`tf.contrib.keras` or external)       | Integrated as `tf.keras`             |\n",
        "| **Model Building**         | Low-level, requires manual session management   | High-level, intuitive Keras APIs     |\n",
        "| **Session & Placeholders** | Required (`tf.Session()`, `tf.placeholder()`)   | Removed (no sessions needed)         |\n",
        "| **Control Flow**           | TensorFlow-specific ops (`tf.while_loop`, etc.) | Native Python control flow works     |\n",
        "| **API Cleanup**            | Many redundant/experimental APIs                | Deprecated or removed unused APIs    |\n",
        "| **Distribution Strategy**  | Limited and hard to configure                   | Unified `tf.distribute` API          |\n",
        "| **Compatibility**          | Many breaking changes between minor versions    | More stable and consistent API base  |\n",
        "\n",
        "\n",
        "2. How do you install TensorFlow 2.0?\n",
        "\n",
        "Set up a virtual environment (Recommended)\n",
        "         python -m venv tf2_env\n",
        "         source tf2_env/bin/activate      \n",
        "\n",
        "Install TensorFlow 2.0.0:\n",
        "       pip install tensorflow==2.0.0\n",
        "\n",
        "After installation, open a Python shell or script and run:\n",
        "       import tensorflow as tf\n",
        "       print(tf.__version__)\n",
        "\n",
        "\n",
        "3. What is the primary function of the tf.function in TensorFlow 2.0?\n",
        "\n",
        "Purpose of tf.function\n",
        "TensorFlow 2.0 uses eager execution by default, which makes it easy to write and debug code. However, eager execution is slower compared to graph execution.\n",
        "\n",
        "The @tf.function decorator allows you to:\n",
        "\n",
        "Compile a Python function into a TensorFlow computational graph\n",
        "\n",
        "Boost performance (often significantly)\n",
        "\n",
        "Enable deployment of models to production or mobile (via SavedModel, TensorFlow Lite, etc.)\n",
        "\n",
        "4. What is the purpose of the Model class in TensorFlow 2.0?\n",
        "\n",
        "The purpose of the Model class in TensorFlow 2.0 is to serve as the core class for building and training machine learning models using the high-level Keras API.\n",
        "\n",
        "It is part of tf.keras and provides a clean and flexible way to:\n",
        "\n",
        "Define the model architecture\n",
        "\n",
        "Train and evaluate models\n",
        "\n",
        "Make predictions\n",
        "\n",
        "Export models\n",
        "\n",
        "\n",
        "In TensorFlow 2.0, Model comes in two main forms:\n",
        "\n",
        "Functional API (tf.keras.Model)\n",
        "\n",
        "Subclassing API (custom models via class inheritance)\n",
        "\n",
        "5. How do you create a neural network using TensorFlow 2.0?\n",
        "\n",
        "Creating a neural network in TensorFlow 2.0 is straightforward, especially using the Keras API (tf.keras), which is built into TensorFlow.\n",
        "\n",
        "Here’s a step-by-step guide to building a simple feedforward neural network (fully connected) for classification.\n",
        "\n",
        "Creating a Neural Network in TensorFlow 2.0:\n",
        "\n",
        "Import Required Modules\n",
        "        import tensorflow as tf\n",
        "        from tensorflow.keras import layers, models\n",
        "\n",
        "Prepare Your Data\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        # Normalize input values to [0, 1]\n",
        "        x_train = x_train / 255.0\n",
        "        x_test = x_test / 255.0\n",
        "\n",
        "Define the Neural Network Model\n",
        "       model = models.Sequential([\n",
        "       layers.Flatten(input_shape=(28, 28)),        # Flatten 2D images to 1D vector\n",
        "       layers.Dense(128, activation='relu'),        # Hidden layer\n",
        "       layers.Dropout(0.2),                         # Dropout for regularization\n",
        "       layers.Dense(10, activation='softmax')       # Output layer (10 classes)\n",
        "       ])\n",
        "\n",
        "Compile the Model\n",
        "      model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "Train the Model\n",
        "      model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
        "\n",
        "Evaluate the Model\n",
        "      model.evaluate(x_test, y_test)\n",
        "\n",
        "Make Predictions\n",
        "     predictions = model.predict(x_test)\n",
        "     print(tf.argmax(predictions[0]))  # Predicted class for the first test image\n",
        "\n",
        "6. What is the importance of Tensor Space in TensorFlow?\n",
        "\n",
        "In TensorFlow, the concept of \"tensor space\" is important for understanding how data is represented, manipulated, and processed in a machine learning model. While the term \"tensor space\" itself isn't commonly used in TensorFlow's official documentation, it can be interpreted through the lens of linear algebra and multidimensional data structures.\n",
        "\n",
        "In mathematics, a tensor space refers to a vector space that consists of tensors of a given rank and shape. It's a generalization of scalar (0D), vector (1D), and matrix (2D) spaces to higher dimensions.\n",
        "\n",
        "In TensorFlow:\n",
        "\n",
        "A tensor is a multidimensional array (like NumPy arrays).\n",
        "\n",
        "A tensor space is the abstract space formed by all possible tensors of a given shape and data type.\n",
        "\n",
        "For example, all 3x3 float32 matrices form a specific tensor space.\n",
        "\n",
        "7. How can TensorBoard be integrated with TensorFlow 2.0?\n",
        "\n",
        "Integrating TensorBoard with TensorFlow 2.0 is quite straightforward and is actually built into the TensorFlow 2.x ecosystem. TensorBoard helps you visualize and debug machine learning models — including metrics like loss/accuracy, model graphs, histograms, images, and more.\n",
        "\n",
        "Import the Required Libraries:\n",
        "    import tensorflow as tf\n",
        "    import datetime  # For timestamping log directories\n",
        "\n",
        "Define Your Model\n",
        "    model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "Setup TensorBoard Callback\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "log_dir: The path where TensorBoard logs will be saved.\n",
        "histogram_freq=1: Records weight histograms every epoch (set to 0 to disable).\n",
        "\n",
        "Train the Model with the Callback\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
        "    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
        "\n",
        "    model.fit(x_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[tensorboard_callback])\n",
        "\n",
        "Launch TensorBoard\n",
        "    tensorboard --logdir logs/fit\n",
        "\n",
        "8.  What is the purpose of TensorFlow Playground?\n",
        "\n",
        "The TensorFlow Playground is an interactive, web-based visualization tool designed to help users understand how neural networks work, particularly in terms of:\n",
        "\n",
        "how they learn patterns,\n",
        "how various hyperparameters affect learning,\n",
        "and how network architecture influences results.\n",
        "\n",
        "Purpose of TensorFlow Playground\n",
        "\n",
        "| **Goal**                               | **Explanation**                                                            |\n",
        "| -------------------------------------- | -------------------------------------------------------------------------- |\n",
        "| 🎓 **Educational Tool**                | Helps beginners grasp fundamental ML concepts visually.                    |\n",
        "| 🧠 **Understand Neural Networks**      | Explore how layers, neurons, and activations impact learning.              |\n",
        "| ⚙️ **Hyperparameter Tuning Demo**      | Test the effects of changing learning rate, activation function, etc.      |\n",
        "| 🧪 **Experimentation Sandbox**         | Try different datasets, architectures, and regularization techniques live. |\n",
        "| 🎯 **Decision Boundary Visualization** | See how the model classifies input space in real-time.                     |\n",
        "\n",
        "\n",
        "9. What is Netron, and how is it useful for deep learning models?\n",
        "\n",
        "Netron is an open-source visualizer for deep learning, machine learning, and neural network models. It allows you to graphically explore model architectures — layer by layer — in a clean and intuitive interface.\n",
        "\n",
        "Netron:\n",
        "A model visualization tool (desktop and web app)\n",
        "Supports many formats: TensorFlow, Keras, PyTorch, ONNX, CoreML, TFLite, and more\n",
        "\n",
        "Why Netron is Useful for Deep Learning:\n",
        "\n",
        "| Use Case                          | Benefit                                                                |\n",
        "| --------------------------------- | ---------------------------------------------------------------------- |\n",
        "| 🔍 **Model Inspection**           | Understand what's inside a model — layer by layer                      |\n",
        "| 🐞 **Debugging**                  | Detect architecture issues like shape mismatches                       |\n",
        "| 🧪 **Model Comparison**           | Compare models side-by-side during experiments or deployment tuning    |\n",
        "| 📦 **Deployment Readiness**       | Verify model architecture before converting formats or deploying       |\n",
        "| 📚 **Education & Documentation**  | Great for teaching neural networks or documenting model design         |\n",
        "| 🔁 **Framework Interoperability** | See how a model from PyTorch looks when exported to ONNX or TensorFlow |\n",
        "\n",
        "10.  What is the difference between TensorFlow and PyTorch?\n",
        "\n",
        "TensorFlow and PyTorch are two of the most popular deep learning frameworks today. Both are powerful, widely adopted, and open-source, but they differ in design philosophy, usability, and ecosystem support.\n",
        "\n",
        "Core Differences between TensorFlow and PyTorc:\n",
        "\n",
        "| Aspect                  | **TensorFlow**                                          | **PyTorch**                                            |\n",
        "| ----------------------- | ------------------------------------------------------- | ------------------------------------------------------ |\n",
        "| **Eager Execution**     | Default in TF 2.x (optional in TF 1.x)                  | Native and default                                     |\n",
        "| **Computation Graph**   | Static (declarative) in TF 1.x, dynamic in TF 2.x       | Dynamic (imperative) — more intuitive                  |\n",
        "| **Ease of Use**         | More verbose, especially in older versions              | More Pythonic and beginner-friendly                    |\n",
        "| **Debugging**           | Harder (TF 1.x), better with TF 2.x and eager execution | Easier with native Python debugging                    |\n",
        "| **Model Building API**  | `tf.keras`, high-level and stable                       | `torch.nn`, very flexible                              |\n",
        "| **Deployment Tools**    | TensorFlow Lite, TensorFlow Serving, TensorFlow\\.js     | TorchScript, ONNX, PyTorch Mobile                      |\n",
        "| **Visualization**       | TensorBoard (very mature and integrated)                | Supports TensorBoard; also uses Visdom or custom tools |\n",
        "| **Community Support**   | Large, especially in production and mobile              | Strong in research and academia                        |\n",
        "| **Speed & Performance** | Comparable; depends on hardware and use case            | Comparable                                             |\n",
        "\n",
        "\n",
        "11. How do you install PyTorch?\n",
        "\n",
        "Installing PyTorch is straightforward and can be done using pip or conda, depending on your environment. Here's a step-by-step guide:\n",
        "\n",
        "Check Your System Configuration\n",
        "Before installing, you should know:\n",
        "\n",
        "Your operating system: Windows, Linux, or macOS\n",
        "\n",
        "Your Python version: python --version (should be ≥ 3.7)\n",
        "\n",
        "Whether you want to use GPU (CUDA) or just CPU\n",
        "\n",
        "Use the Official PyTorch Installation Selector\n",
        "Go to the official site:\n",
        "👉 https://pytorch.org/get-started/locally/\n",
        "\n",
        "It generates the exact install command based on:\n",
        "\n",
        "OS\n",
        "\n",
        "Package manager (pip/conda)\n",
        "\n",
        "Python version\n",
        "\n",
        "CUDA version (or CPU-only)\n",
        "\n",
        "Common Installation Commands\n",
        "\n",
        "🔹 Using pip (recommended for most users)\n",
        "\n",
        "CPU-only:\n",
        "pip install torch torchvision torchaudio\n",
        "\n",
        " Verify Installation\n",
        "    import torch\n",
        "    print(torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "\n",
        "12.  What is the basic structure of a PyTorch neural network?\n",
        "\n",
        "The basic structure of a PyTorch neural network involves defining a class that inherits from torch.nn.Module. This class defines the layers in the __init__ method and the forward pass in the forward method:\n",
        "\n",
        "Import Required Libraries:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "Define the Neural Network Class:\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        \n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(784, 128)   # Fully connected layer: 784 input -> 128 hidden\n",
        "        self.fc2 = nn.Linear(128, 64)    # 128 hidden -> 64 hidden\n",
        "        self.fc3 = nn.Linear(64, 10)     # 64 hidden -> 10 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        x = F.relu(self.fc1(x))          # Apply ReLU activation\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)                  # Output layer (no activation if using CrossEntropyLoss)\n",
        "        return x\n",
        "\n",
        "Create an Instance of the Network:\n",
        "\n",
        "model = SimpleNet()\n",
        "\n",
        "Define Loss Function and Optimizer:\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "13. What is the significance of tensors in PyTorch?\n",
        "\n",
        "Tensors are foundational to PyTorch—they are the core data structure that all neural networks and computations operate on. Here's why they're significant:\n",
        "\n",
        " 1. Building Blocks for Data\n",
        "Tensors are multi-dimensional arrays, similar to NumPy arrays, but with extra capabilities.\n",
        "\n",
        "They can represent:\n",
        "\n",
        "Scalars (0D)\n",
        "\n",
        "Vectors (1D)\n",
        "\n",
        "Matrices (2D)\n",
        "\n",
        "Higher-dimensional data (e.g. images, videos, batches)\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "Copy\n",
        "Edit\n",
        "x = torch.tensor([1.0, 2.0, 3.0])  # 1D tensor (vector)\n",
        "\n",
        "2. Support for GPU Acceleration\n",
        "Unlike NumPy arrays, PyTorch tensors can be moved to and operated on using a GPU.\n",
        "\n",
        "This is essential for training large neural networks efficiently.\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "Copy\n",
        "Edit\n",
        "x = x.to('cuda')  # Move tensor to GPU\n",
        "\n",
        "3. Autograd and Backpropagation\n",
        "Tensors in PyTorch can track gradients if requires_grad=True.\n",
        "\n",
        "This enables automatic differentiation—essential for training via backpropagation.\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "Copy\n",
        "Edit\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "y = x ** 2\n",
        "y.backward()\n",
        "print(x.grad)  # Output: tensor([4.])\n",
        "\n",
        "4. Interoperability with NumPy\n",
        "You can easily convert tensors to and from NumPy arrays, allowing seamless use of existing Python tools.\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "Copy\n",
        "Edit\n",
        "import numpy as np\n",
        "a = np.array([1, 2, 3])\n",
        "t = torch.from_numpy(a)\n",
        "\n",
        "5. Efficiency and Flexibility\n",
        "PyTorch operations on tensors are optimized and vectorized, enabling fast and efficient computation.\n",
        "\n",
        "They support broadcasting, slicing, reshaping, and a rich set of mathematical operations.\n",
        "\n",
        "\n",
        "14. What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch?\n",
        "\n",
        "The difference between torch.Tensor and torch.cuda.Tensor in PyTorch lies in where the data is stored and where computations are performed—either on the CPU or the GPU.\n",
        "\n",
        "1. torch.Tensor (CPU Tensor)\n",
        "This is the default tensor type in PyTorch.\n",
        "\n",
        "Lives and computes on the CPU.\n",
        "\n",
        "Created using\n",
        "\n",
        "x = torch.tensor([1.0, 2.0])  # Stored on CPU\n",
        "print(x.device)  # Output: cpu\n",
        "\n",
        "2. torch.cuda.Tensor (GPU Tensor)\n",
        "Lives and computes on a CUDA-enabled GPU.\n",
        "\n",
        "Allows for fast parallel computation using GPU hardware.\n",
        "\n",
        "Created by moving a tensor to a CUDA device:\n",
        "\n",
        "x_gpu = x.to('cuda')         # or x.cuda()\n",
        "print(x_gpu.device)          # Output: cuda:0\n",
        "\n",
        "Difference:\n",
        "\n",
        "| Feature           | `torch.Tensor` (CPU) | `torch.cuda.Tensor` (GPU)     |\n",
        "| ----------------- | -------------------- | ----------------------------- |\n",
        "| Location          | CPU                  | GPU (CUDA)                    |\n",
        "| Speed             | Slower               | Faster for large computations |\n",
        "| Device            | `cpu`                | `cuda:0`, `cuda:1`, etc.      |\n",
        "| Conversion        | `.to('cuda')`        | `.to('cpu')`                  |\n",
        "| Type (internally) | `torch.FloatTensor`  | `torch.cuda.FloatTensor`      |\n",
        "\n",
        "\n",
        "15. What is the purpose of the torch.optim module in PyTorch?\n",
        "\n",
        "The torch.optim module in PyTorch provides optimization algorithms that are used to update the model's parameters during training.\n",
        "\n",
        "Purpose of torch.optim\n",
        "\n",
        "In neural network training, we want to minimize a loss function by adjusting the model's parameters (weights and biases). This is done using gradient-based optimization.\n",
        "\n",
        "torch.optim contains implementations of common gradient descent-based optimizers like:\n",
        "\n",
        "SGD (Stochastic Gradient Descent)\n",
        "\n",
        "Adam\n",
        "\n",
        "RMSprop\n",
        "\n",
        "Adagrad\n",
        "\n",
        "AdamW\n",
        "\n",
        "16. What are some common activation functions used in neural networks?\n",
        "\n",
        "Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. Without them, a neural network would just be a stack of linear operations—no better than a single-layer model.\n",
        "\n",
        "Here are some of the most common activation functions used in practice:\n",
        "\n",
        "1.ReLU (Rectified Linear Unit)\n",
        "Formula: f(x) = max(0, x)\n",
        "\n",
        "Pros: Simple, efficient, avoids vanishing gradients.\n",
        "\n",
        "Use case: Default choice for hidden layers in most deep networks.\n",
        "\n",
        "import torch.nn.functional as F\n",
        "F.relu(x)\n",
        "\n",
        "2. Sigmoid\n",
        "Formula: f(x) = 1 / (1 + exp(-x))\n",
        "\n",
        "Output range: (0, 1)\n",
        "\n",
        "Use case: Binary classification output or gates in LSTM.\n",
        "\n",
        "Drawbacks: Can cause vanishing gradient problems in deep networks.\n",
        "\n",
        "F.sigmoid(x)  # or torch.sigmoid(x)\n",
        "\n",
        "\n",
        "3. Tanh (Hyperbolic Tangent)\n",
        "Formula: f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
        "\n",
        "Output range: (−1, 1)\n",
        "\n",
        "Use case: Hidden layers, especially in RNNs.\n",
        "\n",
        "Drawbacks: Also suffers from vanishing gradients, but better centered than sigmoid.\n",
        "\n",
        "F.tanh(x)  # or torch.tanh(x)\n",
        "\n",
        "4. Leaky ReLU\n",
        "Formula: f(x) = x if x > 0 else alpha * x (e.g., alpha = 0.01)\n",
        "\n",
        "Improvement over ReLU: Allows a small gradient when x < 0.\n",
        "\n",
        "F.leaky_relu(x, negative_slope=0.01)\n",
        "\n",
        "\n",
        "5. Softmax\n",
        "Formula: f(xi) = exp(xi) / sum(exp(xj))\n",
        "\n",
        "Use case: Converts raw logits into probabilities for multi-class classification.\n",
        "\n",
        "Usually applied in the output layer.\n",
        "\n",
        "F.softmax(x, dim=1)  # dim=1 for batch-wise softmax across classes\n",
        "\n",
        "6. ELU (Exponential Linear Unit)\n",
        "Smooth version of ReLU that can improve learning speed.\n",
        "\n",
        "Helps avoid dead neurons (like ReLU) and maintains mean activations closer to zero.\n",
        "\n",
        "F.elu(x, alpha=1.0)\n",
        "\n",
        "\n",
        "17.  What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch ?\n",
        "\n",
        "n PyTorch, both torch.nn.Module and torch.nn.Sequential are used to define neural network architectures, but they differ in flexibility, structure, and use cases.\n",
        "\n",
        "1. torch.nn.Module — Full Custom Neural Networks\n",
        "The base class for all neural network models in PyTorch.\n",
        "\n",
        "Allows maximum flexibility: you define layers in __init__() and implement the forward logic in forward().\n",
        "\n",
        "Use when:\n",
        "You need custom logic, branching, multiple inputs/outputs, or conditional operations.\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "2. torch.nn.Sequential — Quick Layer Stacking\n",
        "A convenient wrapper for stacking layers in a sequence.\n",
        "\n",
        "Automatically defines forward() as a simple forward pass through each layer in order.\n",
        "\n",
        "Use when:\n",
        "\n",
        "Your model is a straight stack of layers without any branching or complex behavior.\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "\n",
        "18. How can you monitor training progress in TensorFlow 2.0?\n",
        "\n",
        "Monitoring training progress in TensorFlow 2.0 is essential for diagnosing model performance, tuning hyperparameters, and ensuring the training process behaves as expected.\n",
        "\n",
        "Here are several common and effective ways to monitor training progress:\n",
        "\n",
        "1. Using verbose in model.fit()\n",
        "When training with Keras's model.fit(), you can set verbose=1 (default) to print real-time progress:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "Shows loss and metric values after each epoch.\n",
        "\n",
        "You can also use verbose=2 (epoch-level logs) or verbose=0 (no logs).\n",
        "\n",
        "2. Using Callbacks (e.g. TensorBoard, ModelCheckpoint)\n",
        "Callbacks give you more control over training and allow advanced monitoring.\n",
        "\n",
        "TensorBoard (Most common for visualization)\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "tensorboard_callback = TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n",
        "Run TensorBoard from terminal:\n",
        "\n",
        "bash\n",
        "Copy\n",
        "Edit\n",
        "tensorboard --logdir=./logs\n",
        "View in browser: http://localhost:6006\n",
        "\n",
        "What you get:\n",
        "\n",
        "Live graphs of loss, accuracy\n",
        "\n",
        "Histograms of weights\n",
        "\n",
        "Learning rate tracking\n",
        "\n",
        "Model graph visualization\n",
        "\n",
        "ModelCheckpoint (Save model based on performance)\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[checkpoint])\n",
        "EarlyStopping (Stop training when performance stalls)\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[early_stop])\n",
        "3. Plotting Loss & Accuracy Manually\n",
        "Store the training history and plot it:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
        "\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "You can also plot accuracy or other metrics the same way.\n",
        "\n",
        "4. Custom Callback (Advanced Monitoring)\n",
        "Create a custom callback to log specific events or metrics:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class PrintEpochCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"Epoch {epoch+1}: loss = {logs['loss']:.4f}, val_loss = {logs['val_loss']:.4f}\")\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[PrintEpochCallback()])\n",
        "\n",
        "\n",
        "19.  How does the Keras API fit into TensorFlow 2.0?\n",
        "\n",
        "In TensorFlow 2.0, the Keras API is the official high-level API for building and training deep learning models.\n",
        "\n",
        "Keras in TensorFlow 2.0: Integrated & Native\n",
        "In TensorFlow 2.0 and later:\n",
        "\n",
        "tf.keras is built-in — it's not a separate package anymore.\n",
        "\n",
        "It provides a clean, user-friendly interface for defining models, layers, and training workflows.\n",
        "\n",
        "Why Use tf.keras?\n",
        "Feature\tBenefit\n",
        "High-level abstraction\tEasy model building and training\n",
        "Native TF integration\tFully compatible with tf.data, tf.function, tf.GradientTape, etc.\n",
        "Production-ready\tWorks with TensorFlow Serving, TFLite, TPU, etc.\n",
        "Community-supported\tActively maintained by TensorFlow team\n",
        "\n",
        "Key Components of tf.keras\n",
        "Component\tDescription\n",
        "tf.keras.layers\tPre-built layers like Dense, Conv2D, LSTM, etc.\n",
        "tf.keras.models\tModel building APIs: Sequential, Model (Functional)\n",
        "tf.keras.losses\tLoss functions like BinaryCrossentropy, MSE\n",
        "tf.keras.optimizers\tOptimizers like Adam, SGD\n",
        "tf.keras.metrics\tMetrics like Accuracy, AUC, etc.\n",
        "tf.keras.callbacks\tTools like EarlyStopping, TensorBoard, etc.\n",
        "\n",
        "\n",
        "20. What is an example of a deep learning project that can be implemented using TensorFlow 2.0?\n",
        "\n",
        "Here's an example of a practical deep learning project you can implement using TensorFlow 2.0:\n",
        "\n",
        "Project: Handwritten Digit Recognition with MNIST (Using CNN + TensorFlow 2.0)\n",
        "\n",
        "Objective:\n",
        "Train a Convolutional Neural Network (CNN) to classify handwritten digits (0–9) from the MNIST dataset.\n",
        "\n",
        "Step 1: Import Required Libraries\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Step 2: Load and Preprocess Data\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Reshape to add channel dimension (28x28x1)\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "Step 3: Build the CNN Model\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "Step 4: Compile the Model\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "Step 5: Train the Model\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "history = model.fit(x_train, y_train, epochs=5,\n",
        "                    validation_split=0.1, batch_size=64)\n",
        "\n",
        "Step 6: Evaluate and Plot Results\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Plot training & validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='train acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "Result:\n",
        "You’ll typically get ~99% accuracy on the MNIST test set with this setup.\n",
        "\n",
        "21. What is the main advantage of using pre-trained models in TensorFlow and PyTorch?\n",
        "\n",
        "The main advantage of using pre-trained models in TensorFlow or PyTorch is that they allow you to leverage learned features from large, high-quality datasets—saving time, compute resources, and data.\n",
        "\n",
        "Key Benefits of Using Pre-trained Models:\n",
        "\n",
        "| Benefit                        | Explanation                                                                                                                                 |\n",
        "| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| 🔁 **Transfer Learning**       | You can fine-tune a model trained on a large dataset (like ImageNet) to solve a related but smaller task (like classifying medical images). |\n",
        "| ⚡ **Faster Development**       | Training deep models from scratch takes hours to days—pre-trained models can get you started in minutes.                                    |\n",
        "| 🧠 **Better Performance**      | Pre-trained models often achieve higher accuracy than training from scratch, especially with limited data.                                  |\n",
        "| 💾 **Lower Data Requirements** | You don’t need millions of images to train from scratch; you can fine-tune with just a few thousand.                                        |\n",
        "| 🧰 **Built-in Architectures**  | Models like ResNet, BERT, GPT, MobileNet, and EfficientNet are pre-trained and ready to use.                                                |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PBgyB6l7g8LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. How do you install and verify that TensorFlow 2.0 was installed successfully.\n",
        "\n",
        "# Step 1:Install TensorFlow 2.x\n",
        "#Option A: Using pip (recommended)\n",
        "#Open a terminal or command prompt and run:\n",
        "\n",
        "pip install tensorflow\n",
        "\n",
        "# Verify Installation in Python\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "#Step 3: Test a Simple Operation\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Simple test\n",
        "hello = tf.constant(\"Hello, TensorFlow!\")\n",
        "print(hello.numpy())\n",
        "\n",
        "# Step 4: (Optional) Check for GPU Support\n",
        "\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "id": "AmS-8jdvZmkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. How can you define a simple function in TensorFlow 2.0 to perform addition?\n",
        "\n",
        "#In TensorFlow 2.0, you can define a simple addition function using either TensorFlow operations or Python functions.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def add_tensors(a, b):\n",
        "    return tf.add(a, b)\n",
        "\n",
        "#Example :\n",
        "\n",
        "x = tf.constant(10)\n",
        "y = tf.constant(5)\n",
        "\n",
        "result = add_tensors(x, y)\n",
        "print(\"Result:\", result.numpy())  # Output: 15\n"
      ],
      "metadata": {
        "id": "f8PGKIofb5d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. How can you create a simple neural network in TensorFlow 2.0 with one hidden layer ?\n",
        "\n",
        "# We can create a simple neural network with one hidden layer in TensorFlow 2.0 using the high-level tf.keras API.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)),  # Hidden layer\n",
        "    tf.keras.layers.Dense(10, activation='softmax')                    # Output layer\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Dummy data: 1000 samples, each with 784 features\n",
        "x_train = np.random.rand(1000, 784).astype('float32')\n",
        "y_train = np.random.randint(0, 10, size=(1000,))\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_h8PNIlrsPkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.  How can you visualize the training progress using TensorFlow and Matplotlib ?\n",
        "\n",
        "# Train the Model and Store History\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    epochs=10,\n",
        "                    batch_size=32)\n",
        "\n",
        "# Plot with Matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "69ahmzgys2sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.  How do you install PyTorch and verify the PyTorch installation ?\n",
        "\n",
        "\"\"\"Go to the official PyTorch install page:\n",
        "https://pytorch.org/get-started/locally/\n",
        "\n",
        "Choose:\n",
        "\n",
        "OS (e.g., Windows, Linux, macOS)\n",
        "\n",
        "Package manager (pip or conda)\n",
        "\n",
        "Python version\n",
        "\n",
        "CUDA version (for GPU), or \"CPU\" if you don’t have a GPU\"\"\"\n",
        "\n",
        "pip install torch torchvision torchaudio\n",
        "\n",
        "\n",
        "# Verify Installation\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "1bafNrNqtanq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.  How do you create a simple neural network in PyTorch?\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)  # Hidden layer\n",
        "        self.fc2 = nn.Linear(64, 10)   # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))       # Activation function\n",
        "        x = self.fc2(x)               # No softmax; CrossEntropyLoss handles it\n",
        "        return x\n",
        "\n",
        "model = SimpleNet()\n",
        "criterion = nn.CrossEntropyLoss()        # Suitable for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Simulate 100 samples of flattened 28x28 images\n",
        "x_train = torch.rand(100, 784)\n",
        "y_train = torch.randint(0, 10, (100,))  # Labels from 0 to 9\n",
        "\n",
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()            # Reset gradients\n",
        "    outputs = model(x_train)         # Forward pass\n",
        "    loss = criterion(outputs, y_train)  # Compute loss\n",
        "    loss.backward()                  # Backward pass\n",
        "    optimizer.step()                 # Update weights\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "PqePvBSpu_71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.  How do you define a loss function and optimizer in PyTorch?\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = MyNeuralNet()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "IeabgOxSvaP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.  How do you implement a custom loss function in PyTorch?\n",
        "\n",
        "#Custom Loss as a Function\n",
        "\n",
        "import torch\n",
        "\n",
        "def custom_mse_loss(output, target):\n",
        "    return torch.mean((output - target) ** 2)\n",
        "\n",
        "#Subclassing nn.Module (Recommended for integration with PyTorch APIs)\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        return torch.mean((output - target) ** 2)\n"
      ],
      "metadata": {
        "id": "rKkfMgGQxEFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.  How do you save and load a TensorFlow model?\n",
        "\n",
        "# Save the Model\n",
        "\n",
        "model.save('my_model')\n",
        "\n",
        "#  Load the Model\n",
        "\n",
        "loaded_model = tf.keras.models.load_model('my_model')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4g8yhBzZx3LO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}